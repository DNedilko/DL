{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторна Робота 1\n",
    "### З дисципліни \"Методи глибинного навчання\"\n",
    "##### Виконала Неділько Дарина, група КМ-91\n",
    "**Тема:** «Розробка  програмного  забезпечення  для  реалізації  ймовірнісної\n",
    "нейронної мережі PNN»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Завдання:** розробити програмне забезпечення для реалізації мережі PNN. Побудувати мережу PNN, призначену для класифікації об’єктів на А та В.\n",
    "\n",
    "**Теоретичні відомості**\n",
    "Мережі PNN є одним з типів ймовірнісних НМ. Зазвичай вони використовуються для задач класифікації. У задачах класифікації виходи інтерпретуються як ймовірності того, що образ належить деякому класу. Під час розв'язання цього типу задачі розрахунок щільності ймовірності відбувається за допомогою методу ядерних оцінок. Ідея цього методу полягає у тому, що якщо спостереження ведеться у певній точці простору ознак класів, то це означає, що в у цій точці є деяка ненульова щільність імовірності. Чим ближче до точки - тим більша щільність. сумарну функціональну оцінку щільності ймовірності можна розрахувати як суму вказанх функцій.\n",
    "Оцінка функції щільності ймовірності виставляється на основі навчальних образів з використанням методу Парцена. При цьому застосовується вагова функція(ядро), що має центр у точці, яка являє собою навчальний образ. Найчастіше в якості ядра використовують функцію Гауса. Мережа складається із чотирьох шарів нейронів, кількість яких визначається структурою навчальних даних. Кількість вхідних нейронів дорівнює кількості ознак класу. Кількість елементів ШО дорівнює кількості навчальних образів. Вхідний шар і ШО становлять повнозв'язну структуру. Кількість елементів шару додавання(ШД) дорівнює кількості класів. Елемент ШО зв'язаний тільки з тим елементом ШД, якому відповідає клас образу.\n",
    "Спробуємо розібратися на прикладі завдань для другої лабораторної роботи. Для навчання мережі нам подано чотири образи:\n",
    "\n",
    "| x1 | x2 | y |\n",
    "| :- | -: | :-: |\n",
    "| 1 | 1 | A |\n",
    "| 1 | 1 | A |\n",
    "| 2 | 1 | A |\n",
    "| 10 | 10 | B |\n",
    "\n",
    "\n",
    "Для цього випаду у вхідному шарі у нас буде 2 нейрони, адже вхідни параметри у нас 2: х1 та х2, у шарі образів у нас буде 4 нейрони, оскільки навчальних образів у нас 4. В шарі додавання у буде 2 нейрони, оскільки у нас лше два класи А та В.\n",
    "Активність j-того нейрона ШО $\\theta_j^o$ розраховується так: $$\\theta_j^o = \\sum_{i=1}^N{exp(\\frac{-(w_{i,j}-x{i})^2}{\\sigma^2})},$$\n",
    "де x - невідомий образ, хі - іта компонента невідомого образу, N - кількість компонент вхідного вектору-образу, $\\sigma$ - радіус функції Гауса.\n",
    "Для зв'язків, що входять в елемент ШО, вагові коефіціенти встановлюютьяс такими ж, як складові частини відповідного навчального прикладу. Тобто усі параметри мережі PNN безпосередньо визначаються навчальними прикладами. Вагові коефіціенти зв'язків, що входять в ШД та вихідний еоемент, дорівнюють 1. Нейронам ШД характерна лінійна функція активацї. Активність j-того нейрону ШД $\\theta_j^S$ становить: $$\\theta_j^S = \\frac{\\sum_{i=1}^N{\\theta_j^o}}{N},$$де N - кількість нейронів ШО, що пов'язані з j-м нейроном ШД, $\\theta_j^o$ - активність i-го нейрону ШО, що пов'язаний з j-тим нейроном ШД.\n",
    "Значення активності нейрона ШД визначає ймовірність віднесення вхідного образу до класу, що відповідає цьому нейрону. Вихідний елемент визначає тільки нейрон з максимальною активністю. Для процесу навчання мережі PNN важливою є наявність тільки одного керувального параметру навчання - $\\sigma$ - радіусу функції Гауса. PNN малочутливі до величини цього радіусу, тому я візьму сигма = 1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для образу x=[1 0] визначено клас A\n",
      "Для образу x=[3 4] визначено клас B\n",
      "Для образу x=[12 10] визначено клас B\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "# пишемо модель\n",
    "def PNN(df,clasPoint):\n",
    "    # визначаємо класи\n",
    "    Classes = df.groupby('y')\n",
    "    # визначаємо кількість класів\n",
    "    ClassPat = len(Classes)\n",
    "    # задаємо словник, щоб зберігати у ньому пораховані у ШД суми\n",
    "    Sums = dict()\n",
    "    # задаємо кількість нейронів у вхідному шарі\n",
    "    EN = df.shape[1]-1\n",
    "\n",
    "    sigma = 0.5\n",
    "\n",
    "#     масив хів\n",
    "    Xs = df.drop(['y'],axis=1).values\n",
    "\n",
    "    img=0\n",
    "    for i in Classes.groups.keys():\n",
    "        Sums[i] = 0\n",
    "        #   визначаємо кількість нейронів ШО. що відповідають нейронам в ШД для iтого класу\n",
    "        ClassPoins = len(Classes.get_group(i))\n",
    "        theta_s = 0\n",
    "\n",
    "        for j in range(0,ClassPoins):\n",
    "            # визначаємо значення функції активації для шару ШО\n",
    "            theta_o = math.exp(-((Xs[img][0] - clasPoint[0])**2+(Xs[img][1] - clasPoint[1])**2)/(sigma**2))\n",
    "\n",
    "#             визначаємо значення функції активації для шару ШС з кожною ітерацією\n",
    "            theta_s += theta_o\n",
    "        Sums[i] = theta_s/ClassPoins\n",
    "        img+=1\n",
    "\n",
    "    # дізаємося для якого класу більша ймовірність співпадіння\n",
    "    res = max(Sums,key = Sums.get)\n",
    "\n",
    "    print(f'Для образу x={clasPoint} визначено клас {res}')\n",
    "\n",
    "\n",
    "# задаємо навчальні образи\n",
    "train = {\n",
    "    'x1' : [1,1,2,10],\n",
    "    'x2' : [1,2,1,10],\n",
    "    'y' : ['A','A','A','B']\n",
    "}\n",
    "\n",
    "train_df = pd.DataFrame(data=train)\n",
    "\n",
    "# розпізнавання\n",
    "t = np.array([[1,0],[3,4],[12,10]])\n",
    "\n",
    "for el in t:\n",
    "    PNN(train_df,el)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Висновки:** У ході лабораторної роботи було досліджено будову мережі PNN, визначено нейрони у кожному шарі мережі для нашого випадку та натреновано."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}